{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import platform\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def random_color():\n",
    "    return (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "def get_font():\n",
    "    font_size=50\n",
    "    if platform.system() == \"Darwin\":\n",
    "        font = ImageFont.truetype(\"AppleGothic.ttf\", size=font_size)\n",
    "    elif platform.system() == \"Windows\":\n",
    "        font = ImageFont.trutype(\"malgun.ttf\", size=font_size)\n",
    "    else:\n",
    "        font = ImageFont.load_default(size=font_size)\n",
    "    \n",
    "    return font\n",
    "\n",
    "\n",
    "# function\n",
    "weights_path = \"yolo실습자료/yolov3.weights\"\n",
    "config_path = \"yolo실습자료/yolov3.cfg\"\n",
    "names_path = \"yolo실습자료/coco_korean.names\"\n",
    "\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "with open(names_path, 'r', encoding='utf-8') as file:\n",
    "    label_list = file.read().strip().split('\\n')\n",
    "    \n",
    "#print(net, label_list)\n",
    "\n",
    "###################################################\n",
    "# Open AI\n",
    "###################################################\n",
    "def request_gpt(image_array):\n",
    "    endpoint = \"{}openai/deployments/{}/chat/completions?api-version=2025-01-01-preview\".format(OPENAI_ENDPOINT, DEPLOYMENT_NAME)\n",
    "    headers = {\n",
    "        \"Content-Type\" : \"application/json\", \n",
    "        \"api-key\" : OPENAI_API_KEY\n",
    "    }\n",
    "\n",
    "\n",
    "    # numpy -> PIL\n",
    "    image = Image.fromarray(image_array)\n",
    "    # PIL -> 바이너리\n",
    "    buffered_io = io.BytesIO()\n",
    "    image.save(buffered_io, format='png')\n",
    "    # Base64로 인코딩\n",
    "    base64_image = base64.b64encode(buffered_io.getvalue()).decode('utf-8')\n",
    "    #print(base64_image[:100])\n",
    "\n",
    "    message_list = list()\n",
    "\n",
    "    message_list.append({\n",
    "        \"role\" : \"system\", \n",
    "        \"content\" : [{\n",
    "            \"type\" : \"text\", \n",
    "            \"text\" : \"\"\"\n",
    "            너는 사진 속제서 감지된 물체를 분석하는 봇이야.\n",
    "            무조건 분석 결과를 한국어로 답변해줘\n",
    "            \"\"\"\n",
    "        }],\n",
    "        \n",
    "    })\n",
    "\n",
    "    message_list.append({\n",
    "        \"role\" : \"user\", \n",
    "        \"content\" : [{\n",
    "            \"type\" : \"text\", \n",
    "            \"text\" : \"\"\"\n",
    "            너는 물체를 감지하는 YOLO 모델이야.\n",
    "            이 사진에서 감지된 물체데 대해 감지 확률과 함께 자세한 설명을 붙여줘.\n",
    "            반드시 감지된 물체, 바운딩박스,안에 있는 물체에 대해서만 설명해줘\n",
    "            부연 설명은 필요없고 감지왼 물체에 대해서만 설명해줘야 해\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"image_url\",\n",
    "            \"image_url\" : {\n",
    "                \"url\" : \"data:image/png;base64,{}\".format(base64_image)\n",
    "            }\n",
    "        }],\n",
    "        \n",
    "    })\n",
    "\n",
    "    body = {\n",
    "        \"messages\" : message_list,\n",
    "        \"temperature\" : 0.7,\n",
    "        \"top_p\" : 0.95,\n",
    "        \"max_tokens\" : 16000\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "\n",
    "    print(response.status_code, response.text)\n",
    "\n",
    "    if response.status_code ==200:\n",
    "        response_json = response.json()\n",
    "        content = response_json['choices'][0]['message']['content']\n",
    "    else:\n",
    "        content = response.text\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "#####################################\n",
    "# TTS\n",
    "#####################################\n",
    "def request_tts(text):\n",
    "    endpoint = SPEECH_ENDPOINT\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\" : SPEECH_API_KEY,\n",
    "        \"Content-Type\" : \"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\" : \"audio-16khz-128kbitrate-mono-mp3\"\n",
    "    }\n",
    "    body = f\"\"\" \n",
    "        <speak version='1.0' xml:lang='ko-KR'>\n",
    "            <voice xml:lang='en-US' xml:gender='Female' name='ko-KR-GookMinNeural'>\n",
    "                <prosody rate=\"50%\">\n",
    "                    {text}\n",
    "                </prosody>\n",
    "            </voice>\n",
    "        </speak>\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, data=body)\n",
    "    print(response.status_code, response.text)\n",
    "\n",
    "    if response.status_code ==200:\n",
    "        file_name = \"response_audio.wav\"\n",
    "\n",
    "        with open(file_name, \"wb\") as audio_file:\n",
    "            audio_file.write(response.content)\n",
    "        \n",
    "        return file_name\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def detect_objects(image):\n",
    "    \n",
    "    drawn_image = Image.fromarray(image.copy())\n",
    "    draw = ImageDraw.Draw(drawn_image)\n",
    "    \n",
    "    # 이미지의 height, width 를 뽑아낸다.\n",
    "    height , width = image.shape[:2]\n",
    "    #print(height, width)\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob=blob)\n",
    "    # 블롭을 생성하고 전방향 전파 진행.\n",
    "    layer_name_list = net.getLayerNames()\n",
    "    output_layer_list  = [layer_name_list[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    # yolo_82, yolo_94, yolo_102 총 3개의 레이어들이 예측을 진행. detection_list에는 총 3개의 예측이 있음.\n",
    "    detection_list = net.forward(output_layer_list)\n",
    "    \n",
    "    bounding_box_list = list()\n",
    "    confidence_list = list()\n",
    "    class_index_list = list()\n",
    "\n",
    "    for output in detection_list:\n",
    "        # output : 각 레이어의 예측 정보.\n",
    "        for detection in output: \n",
    "            # detection : 총 85개. x, y, w, h, confidence + 총 80개의 names정보.\n",
    "            score_list = detection[5:] \n",
    "            class_index = np.argmax(score_list)\n",
    "            confidence = score_list[class_index]\n",
    "            if confidence > 0:\n",
    "                #print(class_index, label_list[class_index], confidence)\n",
    "                bounding_box = detection[:4] * np.array([width, height, width, height])\n",
    "                center_x, center_y, w, h = bounding_box.astype('int')\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                if x < 0:\n",
    "                    x = 0\n",
    "                if y < 0:\n",
    "                    y = 0\n",
    "                \n",
    "                #print(x, y, w, h)\n",
    "\n",
    "                bounding_box_list.append([x, y, w, h])\n",
    "                confidence_list.append(confidence)\n",
    "                class_index_list.append(class_index)\n",
    "\n",
    "                \n",
    "                # draw.rectangle((x, y, x + w, y + h), outline=(0, 255, 0), width=3)\n",
    "                # draw.text((x + 5, y + 5), text=\"{} : {:0.2f}%\".format(label_list[class_index], confidence * 100), fill=(255, 0, 0))\n",
    "            # print(detection[:5])\n",
    "            # print(detection[5:])\n",
    "\n",
    "    # 중복 제거\n",
    "    extracted_index_list = cv2.dnn.NMSBoxes(bounding_box_list, confidence_list, 0.5, 0.4)\n",
    "    #print(class_index_list, extracted_index_list)\n",
    "\n",
    "\n",
    "    for extracted_index in extracted_index_list:\n",
    "        x, y, w, h = bounding_box_list[extracted_index]\n",
    "        confidence = confidence_list[extracted_index]\n",
    "        class_index = class_index_list[extracted_index]\n",
    "        label = label_list[class_index]\n",
    "\n",
    "        color = random_color()\n",
    "        #print(label, x, y, w, h, confidence)\n",
    "\n",
    "\n",
    "        draw.rectangle((x, y, x + w, y + h), outline=color, width=3)\n",
    "        draw.text((x + 5, y + 5), text=\"{} : {:0.2f}%\".format(label_list[class_index], confidence * 100), fill=color, font=get_font())\n",
    "\n",
    "    # print(image)\n",
    "    \n",
    "    return drawn_image\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    def stream_webcam(image):\n",
    "        # detect function.\n",
    "        drawn_image = detect_objects(image)\n",
    "        return drawn_image\n",
    "    \n",
    "    def click_capture(image):\n",
    "        return image\n",
    "    \n",
    "    def click_send_gpt(image_array, histories):\n",
    "        content = request_gpt(image_array)\n",
    "        histories.append({\"role\" : \"user\", \"content\" : gr.Image(label=\"감지화면\", value=image_array)})\n",
    "        histories.append({\"role\" : \"assistant\", \"content\" : content})\n",
    "\n",
    "        return histories\n",
    "    \n",
    "    def change_chatbot(histories):\n",
    "        #print(histories)\n",
    "        content = histories[-1]['content']\n",
    "        print(content)\n",
    "        pattern = r'[^가-힣a-zA-Z\\s%,\\.\\d]'\n",
    "        cleaned_content = re.sub(pattern, '', content)\n",
    "        print(cleaned_content)\n",
    "\n",
    "        # tts\n",
    "        file_name = request_tts(cleaned_content)\n",
    "        return file_name\n",
    "    \n",
    "    with gr.Row():\n",
    "        # webcam, stream output image, capture image component\n",
    "\n",
    "        webcam_input = gr.Image(label=\"실시간 화면\", sources=\"webcam\", width=480, height=270, mirror_webcam=False)\n",
    "        output_image = gr.Image(label=\"검출 화면\", type=\"pil\", interactive=False)\n",
    "        output_capture_image = gr.Image(label=\"캡쳐 화면\", interactive=False)\n",
    "\n",
    "\n",
    "    with gr.Row():\n",
    "        # 캡쳐 버튼, 분석 버튼\n",
    "        capture_button = gr.Button(\"화면 캡쳐\")\n",
    "        send_gpt_button = gr.Button(\"GPT로 전송\")\n",
    "        \n",
    "\n",
    "    with gr.Column():\n",
    "        # 챗봇\n",
    "        # 분석 내용을 읽어주는 TTS\n",
    "        chatbot = gr.Chatbot(label=\"분석 결과\", type=\"messages\")\n",
    "        chatbot_audio = gr.Audio(label=\"GPT\", interactive=False, autoplay=True) \n",
    "\n",
    "    \n",
    "    webcam_input.stream(stream_webcam, inputs=[webcam_input], outputs=[output_image])\n",
    "    capture_button.click(click_capture, inputs=[output_image], outputs=[output_capture_image])\n",
    "    send_gpt_button.click(click_send_gpt, inputs=[output_capture_image, chatbot], outputs=[chatbot])\n",
    "    chatbot.change(change_chatbot, inputs=[chatbot], outputs=[chatbot_audio])\n",
    "\n",
    "demo.launch()\n",
    "image = cv2.imread(\"./yolo실습자료/traffic-7033509_1280.jpg\")\n",
    "# detect_objects(image)\n",
    "# request_gpt(image)\n",
    "#request_tts(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.12312312  4.246246    9.639369   16.492492  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  9, 16])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "bounding_box = [1,2,3,4] * np.array([1.123123123,2.123123,3.213123,4.123123])\n",
    "print(bounding_box)                                     \n",
    "bounding_box.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
